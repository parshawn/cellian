{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd70fd55",
   "metadata": {},
   "source": [
    "# STATE Transition Model Training on SCP1064\n",
    "\n",
    "This notebook adapts the official STATE training notebook  for the custom Perturb-CITE-seq dataset (SCP1064).\n",
    "\n",
    "The workflow is as follows:\n",
    "1.  **Setup**: Define file paths and import libraries.\n",
    "2.  **Data Preprocessing**:\n",
    "    * Load RNA expression and metadata from your CSVs.\n",
    "    * Create a single `AnnData` object.\n",
    "    * Apply your custom metadata processing (filling \"CTRL\", creating 'perturbation' column).\n",
    "    * Apply the standard preprocessing from the reference: normalize to 10k counts and log-transform .\n",
    "    * Calculate and store 2000 Highly Variable Genes (HVGs).\n",
    "    * Save the final processed data as a `.h5ad` file.\n",
    "3.  **Create TOML Config**:\n",
    "    * Define a train/val/test split by holding out a subset of perturbations from one cellular context, similar to the reference's 'few-shot' setup[cite: 252, 281].\n",
    "    * This split is saved to a `.toml` file.\n",
    "4.  **Install & Train**:\n",
    "    * Change to your local `state` repo directory.\n",
    "    * Install dependencies using `uv`.\n",
    "    * Run the `state tx train` command, pointing to our new data and TOML config.\n",
    "5.  **Predict**:\n",
    "    * Run inference and evaluation on the holdout test set[cite: 449]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ca525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import toml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- User-defined Paths ---\n",
    "# Directory for your data\n",
    "data_dir = \"/home/nebius/cellian/data/perturb-cite-seq/SCP1064\"\n",
    "\n",
    "# Input files\n",
    "meta_path = f\"{data_dir}/metadata/RNA_metadata.csv\"\n",
    "rna_csv = f\"{data_dir}/expression/RNA_expression_subset1a.csv\"\n",
    "protein_csv = f\"{data_dir}/expression/Protein_expression.csv\" # Note: The reference ST model uses RNA profiles [cite: 14]\n",
    "\n",
    "# Path to your local clone of the STATE repo\n",
    "state_repo_dir = \"/home/nebius/ST-Tahoe\"\n",
    "\n",
    "# Output files\n",
    "processed_data_dir = f\"{data_dir}/processed\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "output_adata_path = f\"{processed_data_dir}/scp1064_processed.h5ad\"\n",
    "output_toml_path = f\"{state_repo_dir}/scp1064_split.toml\" # Save TOML config in the repo dir for easy access\n",
    "\n",
    "CELL_TYPE_COLUMN = 'condition' \n",
    "PERTURBATION_COLUMN = 'perturbation'\n",
    "CONTROL_LABEL = 'CTRL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe57f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from /home/nebius/cellian/data/perturb-cite-seq/SCP1064/metadata/RNA_metadata.csv...\n",
      "Loading RNA expression from /home/nebius/cellian/data/perturb-cite-seq/SCP1064/expression/RNA_expression_subset1a.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1376834/3906582621.py:2: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  meta_df = pd.read_csv(meta_path, index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposing RNA matrix to (cells x genes)...\n",
      "Aligning metadata and expression data...\n",
      "Found 27291 common cells.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading metadata from {meta_path}...\")\n",
    "meta_df = pd.read_csv(meta_path, index_col=0)\n",
    "\n",
    "print(f\"Loading RNA expression from {rna_csv}...\")\n",
    "# Genes as index, cells as columns\n",
    "rna_df = pd.read_csv(rna_csv, index_col=0)\n",
    "\n",
    "# Transpose RNA data: AnnData expects (observations x variables), i.e., (cells x genes)\n",
    "print(\"Transposing RNA matrix to (cells x genes)...\")\n",
    "rna_df_t = rna_df.T\n",
    "\n",
    "# Align metadata and expression data\n",
    "print(\"Aligning metadata and expression data...\")\n",
    "common_cells = rna_df_t.index.intersection(meta_df.index)\n",
    "if len(common_cells) == 0:\n",
    "    raise ValueError(\"No common cells found between RNA expression and metadata. Check cell identifiers.\")\n",
    "\n",
    "print(f\"Found {len(common_cells)} common cells.\")\n",
    "rna_df_t = rna_df_t.loc[common_cells]\n",
    "meta_df = meta_df.loc[common_cells]\n",
    "\n",
    "# Create AnnData object\n",
    "adata = ad.AnnData(X=rna_df_t.values, obs=meta_df)\n",
    "adata.var_names = rna_df.index.tolist()\n",
    "del rna_df, rna_df_t, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8f1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying custom metadata processing...\n",
      "Unique perturbations created. Example: ['HLA-B' 'CTRL' 'IFNGR1' 'CDKN1A' 'EMP1']\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying custom metadata processing...\")\n",
    "# 1. Fill NA in 'sgRNA' with 'CTRL'\n",
    "adata.obs['sgRNA'] = adata.obs['sgRNA'].fillna(CONTROL_LABEL)\n",
    "# 2. Create 'perturbation' column by stripping sgRNA number\n",
    "adata.obs[PERTURBATION_COLUMN] = adata.obs['sgRNA'].str.replace(r\"(_\\d+)$\", \"\", regex=True)\n",
    "\n",
    "print(f\"Unique perturbations created. Example: {adata.obs[PERTURBATION_COLUMN].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e730a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standard preprocessing (Normalize total, log1p)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: adata.X seems to be already log-transformed.\n",
      "Calculating 2000 Highly Variable Genes (HVGs)...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please install skmisc package via `pip install --user scikit-misc",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/new_env/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:71\u001b[39m, in \u001b[36m_highly_variable_genes_seurat_v3\u001b[39m\u001b[34m(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskmisc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loess\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'skmisc'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# --- Compute and Store Highly Variable Genes (HVGs) ---\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# The reference notebook computes 2000 HVGs.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalculating 2000 Highly Variable Genes (HVGs)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhighly_variable_genes\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mseurat_v3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Store HVG data in .obsm as a dense array, just like the reference [cite: 228-230]\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStoring HVG array in .obsm[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mX_hvg\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/new_env/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:701\u001b[39m, in \u001b[36mhighly_variable_genes\u001b[39m\u001b[34m(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values)\u001b[39m\n\u001b[32m    699\u001b[39m         sig = signature(_highly_variable_genes_seurat_v3)\n\u001b[32m    700\u001b[39m         n_top_genes = cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m, sig.parameters[\u001b[33m\"\u001b[39m\u001b[33mn_top_genes\u001b[39m\u001b[33m\"\u001b[39m].default)\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_highly_variable_genes_seurat_v3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m cutoff = _Cutoffs.validate(\n\u001b[32m    714\u001b[39m     n_top_genes=n_top_genes,\n\u001b[32m    715\u001b[39m     min_disp=min_disp,\n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m     max_mean=max_mean,\n\u001b[32m    719\u001b[39m )\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m min_disp, max_disp, min_mean, max_mean, n_top_genes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/new_env/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:74\u001b[39m, in \u001b[36m_highly_variable_genes_seurat_v3\u001b[39m\u001b[34m(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     73\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mPlease install skmisc package via `pip install --user scikit-misc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     75\u001b[39m df = pd.DataFrame(index=adata.var_names)\n\u001b[32m     76\u001b[39m data = _get_obs_rep(adata, layer=layer)\n",
      "\u001b[31mImportError\u001b[39m: Please install skmisc package via `pip install --user scikit-misc"
     ]
    }
   ],
   "source": [
    "print(\"Applying standard preprocessing (Normalize total, log1p)...\")\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# --- Compute and Store Highly Variable Genes (HVGs) ---\n",
    "# The reference notebook computes 2000 HVGs.\n",
    "print(\"Calculating 2000 Highly Variable Genes (HVGs)...\")\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat_v3')\n",
    "\n",
    "# Store HVG data in .obsm as a dense array, just like the reference [cite: 228-230]\n",
    "print(\"Storing HVG array in .obsm['X_hvg']...\")\n",
    "hvg_data = adata.X[:, adata.var['highly_variable']]\n",
    "if sparse.issparse(hvg_data):\n",
    "    hvg_data = hvg_data.toarray()\n",
    "adata.obsm['X_hvg'] = hvg_data\n",
    "print(f\"Saving processed AnnData object to {output_adata_path}...\")\n",
    "adata.write(output_adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contexts = adata.obs[CELL_TYPE_COLUMN].unique().tolist()\n",
    "if not all_contexts:\n",
    "    raise ValueError(f\"No contexts found in column '{CELL_TYPE_COLUMN}'. Please check the column name.\")\n",
    "\n",
    "print(f\"Found {len(all_contexts)} cellular contexts: {all_contexts}\")\n",
    "\n",
    "# --- Define the Split ---\n",
    "# We pick ONE context to split for validation and testing\n",
    "# All other contexts will be used entirely for training.\n",
    "context_to_split = all_contexts[0]\n",
    "print(f\"Selected context '{context_to_split}' to create val/test splits from.\")\n",
    "\n",
    "# Get all perturbations present in this context, excluding the control\n",
    "perts_in_context = adata[adata.obs[CELL_TYPE_COLUMN] == context_to_split].obs[PERTURBATION_COLUMN].unique()\n",
    "perts_in_context = [p for p in perts_in_context if p != CONTROL_LABEL]\n",
    "print(f\"Found {len(perts_in_context)} perturbations in '{context_to_split}'.\")\n",
    "\n",
    "# Split these perturbations into validation and test sets (e.g., 50/50 split)\n",
    "val_perts, test_perts = train_test_split(perts_in_context, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Split: {len(val_perts)} validation perts, {len(test_perts)} test perts.\")\n",
    "\n",
    "# --- Build the TOML Config Dictionary ---\n",
    "# This structure follows the reference notebook [cite: 265-281]\n",
    "DATASET_NAME = \"scp1064_dataset\" # An internal name for this dataset\n",
    "\n",
    "config = {\n",
    "    # Map the dataset name to the *directory* containing the .h5ad file\n",
    "    \"datasets\": {\n",
    "        DATASET_NAME: processed_data_dir\n",
    "    },\n",
    "    \n",
    "    # Specify which datasets to use for training\n",
    "    \"training\": {\n",
    "        DATASET_NAME: \"train\"\n",
    "    },\n",
    "    \n",
    "    # Define zero-shot holdouts (none in this setup)\n",
    "    \"zeroshot\": {},\n",
    "    \n",
    "    # Define few-shot holdouts\n",
    "    \"fewshot\": {\n",
    "        # The key is \"dataset_name.context_name\"\n",
    "        f\"{DATASET_NAME}.{context_to_split}\": {\n",
    "            \"val\": list(val_perts),\n",
    "            \"test\": list(test_perts)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Save the TOML File ---\n",
    "print(f\"Saving TOML config to {output_toml_path}...\")\n",
    "with open(output_toml_path, 'w') as f:\n",
    "    toml.dump(config, f)\n",
    "\n",
    "print(\"\\nTOML config generation complete.\")\n",
    "print(f\"--- Config Preview (first 5 perts) ---\")\n",
    "print(f\"[fewshot.{DATASET_NAME}.{context_to_split}]\")\n",
    "print(f\"val = {list(val_perts)[:5]}...\")\n",
    "print(f\"test = {list(test_perts)[:5]}...\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36423c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {state_repo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d625c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add \"cell-load>=0.7.11\" toml pandas scikit-learn scanpy anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run state tx train \\\n",
    "    data.kwargs.toml_config_path=\"{output_toml_path}\" \\\n",
    "    data.kwargs.num_workers=4 \\\n",
    "    data.kwargs.output_space=\"gene\" \\\n",
    "    data.kwargs.batch_col=\"gem_group\" \\\n",
    "    data.kwargs.pert_col=\"{PERTURBATION_COLUMN}\" \\\n",
    "    data.kwargs.cell_type_key=\"{CELL_TYPE_COLUMN}\" \\\n",
    "    data.kwargs.control_pert=\"{CONTROL_LABEL}\" \\\n",
    "    training.max_steps=80000 \\\n",
    "    training.ckpt_every_n_steps=2000 \\\n",
    "    training.batch_size=64 \\\n",
    "    training.lr=1e-3 \\\n",
    "    model.kwargs.cell_set_len=64 \\\n",
    "    model.kwargs.hidden_dim=128 \\\n",
    "    model.kwargs.batch_encoder=True \\\n",
    "    model=state \\\n",
    "    wandb.entity=\"arcinstitute\" \\\n",
    "    wandb.tags=\"[scp1064_run]\" \\\n",
    "    output_dir=\"test_scp1064\" \\\n",
    "    name=\"scp1064_holdout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run state tx predict \\\n",
    "    --output_dir \"test_scp1064/scp1064_holdout\" \\\n",
    "    --checkpoint \"last.ckpt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
